# -*- coding: utf-8 -*-
"""Final_Jypyter_file_(2) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OWZEVzl3pqDq25ZyOeuEjHYJGxH1vkq-
"""

from google.colab import drive
drive.mount('/content/drive')

# Here we did put all the datasets in one dataframe
#including targets and features.
import pandas as pd


file_paths = {
    "sp500_df": "/content/drive/My Drive/AML_Datasets/S&P 500 Futures 2016 24.csv",
    "crudeoil_df": "/content/drive/My Drive/AML_Datasets/Commodities Crude Oil WTI Futures 2016 24.csv",
    "gold_df": "/content/drive/My Drive/AML_Datasets/Commodities Gold Futures 2016 24.csv",
    "eur_df": "/content/drive/My Drive/AML_Datasets/Forex EUR_USD 2016 24.csv",
    "gbp_df": "/content/drive/My Drive/AML_Datasets/Forex GBP_USD 2016 24.csv",
    "cny_df": "/content/drive/My Drive/AML_Datasets/Forex USD_CNY 2016 24.csv",
    "jpy_df": "/content/drive/My Drive/AML_Datasets/Forex USD_JPY 2016 24.csv",
    "usidx_df": "/content/drive/My Drive/AML_Datasets/US Dollar Index 2016 24.csv"
}


dfs = {key: pd.read_csv(path) for key, path in file_paths.items()}

for key, df in dfs.items():
    print(f"\nColumns in '{key}':")
    print(df.columns.tolist())

#handling missing values, backward and forward fill method


# Step 1: Inspect missing values for each dataframe
for key, df in dfs.items():
    print(f"Missing values in {key} before fill:")
    print(df.isnull().sum())

# Step 2: Apply ffill + bfill to each DataFrame in the dictionary
dfs = {key: df.ffill().bfill() for key, df in dfs.items()}

# Step 3: Confirm missing values handled
for key, df in dfs.items():
    print(f"Missing values in {key} after fill:")
    print(df.isnull().sum())

## Using datetime as common column to merge the datasets by setting the index.

for key, df in dfs.items():
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)

#dataset merge using the created dataframe

combined_df = dfs['sp500_df']
for key in ['gold_df', 'crudeoil_df', 'eur_df', 'gbp_df', 'cny_df', 'jpy_df', 'usidx_df']:
    combined_df = combined_df.join(dfs[key], how='inner', rsuffix=f'_{key}')

## data cleaning:- convering to numeric and handling leftover missing values

for col in combined_df.columns:
    if combined_df[col].dtype == object:
        combined_df[col] = (
            combined_df[col].str.replace(',', '')
            .str.replace('%', '')
            .str.replace('M', 'e6')
            .str.replace('B', 'e9')
        )
        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')

combined_df.fillna(method='ffill', inplace=True)

# EDA for modeliing and feature enginering

import matplotlib.pyplot as plt
import seaborn as sns

# Basic statistics
print(combined_df.describe())

# Histograms for numerical features
combined_df.hist(figsize=(15, 10))
plt.tight_layout()
plt.show()

# Correlation matrix
plt.figure(figsize=(18, 18))
sns.heatmap(combined_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Time series plot of S&P 500 closing price
plt.figure(figsize=(12, 6))
plt.plot(combined_df.index, combined_df['Price'])
plt.title('S&P 500 Futures Closing Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.grid(True)
plt.show()

# missing values heatmap
sns.heatmap(combined_df.isnull(), cbar=False, yticklabels=False)
plt.title("Missing Value Heatmap")
plt.show()

# boxplot for outlier detection
plt.figure(figsize=(15, 6))
sns.boxplot(data=combined_df[['Price', 'Open', 'High', 'Low']])
plt.title("Boxplot of Key Financial Features")
plt.show()

#moving average of price to understand trends
combined_df['Rolling_Mean_10'] = combined_df['Price'].rolling(window=10).mean()
plt.figure(figsize=(12, 6))
plt.plot(combined_df['Price'], label='Price')
plt.plot(combined_df['Rolling_Mean_10'], label='10-day Rolling Mean', linestyle='--')
plt.title("S&P 500 with Rolling Mean")
plt.legend()
plt.show()

## feature engineering for fewer important features
# ----- Feature Engineering using S&P 500 Price
combined_df['Momentum_10d'] = combined_df['Price'] - combined_df['Price'].shift(10)
combined_df['Momentum_5d'] = combined_df['Price'] - combined_df['Price'].shift(5)

combined_df['SMA_10'] = combined_df['Price'].rolling(window=10).mean()
combined_df['SMA_5'] = combined_df['Price'].rolling(window=5).mean()
combined_df['SMA_20'] = combined_df['Price'].rolling(window=20).mean()
combined_df['Price_MA5'] = combined_df['Price'].rolling(window=5).mean()

combined_df['Price_volatility_7d'] = combined_df['Price'].rolling(window=7).std()

# --- Feature Ratios
combined_df['Gold_to_SP500'] = combined_df['Gold_Price'] / combined_df['Price']
combined_df['DollarIndex_to_SP500'] = combined_df['usidx_Price'] / combined_df['Price']

# --- RSI Calculation
gain = combined_df['Price'].diff().where(combined_df['Price'].diff() > 0, 0)
loss = -combined_df['Price'].diff().where(combined_df['Price'].diff() < 0, 0)
avg_gain = gain.rolling(14).mean()
avg_loss = loss.rolling(14).mean()
rs = avg_gain / avg_loss
combined_df['RSI'] = 100 - (100 / (1 + rs))

# Create Target Column (future movement)
combined_df['future_price'] = combined_df['Price'].shift(-1)
combined_df['target_movement'] = 0
combined_df.loc[combined_df['future_price'] > combined_df['Price'], 'target_movement'] = 1
combined_df.loc[combined_df['future_price'] < combined_df['Price'], 'target_movement'] = -1
combined_df.drop(columns=['future_price'], inplace=True)  # preventing data leakage

# Preview
combined_df.head()

# setting binary target for daily prediction
# yesterday's clsoing for today == 0, more than yesterday for today == 1, less
# than yesterday's for today == -1.

combined_df['Binary Movement'] = 0
combined_df.loc[combined_df['Price'].diff() > 0, 'Binary Movement'] = 1
combined_df.loc[combined_df['Price'].diff() < 0, 'Binary Movement'] = -1

## Drop unnecessary or high-correlation features to reduce overfitting
## dropping Change%, for checking changes in output

drop_keywords = ['Open', 'High', 'Low', 'Vol.', 'Change %']
columns_to_drop = [col for col in combined_df.columns if any(k in col for k in drop_keywords)]

# Drop price-related columns except the main S&P 500 Price
columns_to_drop += [col for col in combined_df.columns if 'Price' in col and col != 'Price']

combined_df.drop(columns=columns_to_drop, inplace=True)

# Dropping rows with NaNs after feature engineering

combined_df.dropna(inplace=True)

# Preparing for modeling

# 1. Define features (exclude Binary Movement and Price to avoid data leakage)
X = combined_df.drop(['Binary Movement', 'Price'], axis=1)

# 2. Define target variable
y = combined_df['Binary Movement']

# defining the scaler, defining models with parameters, iterating with each scaler,
# using best scaler to get the accuracies

from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import time


start_time = time.time()
# --- Step 1: Split features and target ---
X = combined_df.drop(['Binary Movement', 'Price'], axis=1)
y = combined_df['Binary Movement']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- Step 2: Define scalers and models ---
scalers = {
    'MinMaxScaler': MinMaxScaler(),
    'StandardScaler': StandardScaler(),
    'RobustScaler': RobustScaler(),
    'MaxAbsScaler': MaxAbsScaler()
}

models = {
    'Logistic Regression': (LogisticRegression(solver='saga', max_iter=1000), {
        'model__C': [0.01, 0.1, 1, 10],
        'model__penalty': ['l1', 'l2']
    }),
    'Linear SVM': (LinearSVC(dual=False, max_iter=1000), {
        'model__C': [0.01, 0.1, 1, 10],
        'model__penalty': ['l1', 'l2']
    })
}

results = []

# --- Step 3: Train and Evaluate each model with different scalers ---
for model_name, (model, param_grid) in models.items():
    best_scaler = None
    best_score = -float('inf')
    best_params = None
    best_pipeline = None

    for scaler_name, scaler in scalers.items():
        pipeline = Pipeline([
            ('scaler', scaler),
            ('model', model)
        ])

        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
        grid_search.fit(X_train, y_train)

        if grid_search.best_score_ > best_score:
            best_score = grid_search.best_score_
            best_scaler = scaler_name
            best_params = grid_search.best_params_
            best_pipeline = grid_search.best_estimator_

    # Final model training with best config
    best_pipeline.fit(X_train, y_train)
    y_train_pred = best_pipeline.predict(X_train)
    y_test_pred = best_pipeline.predict(X_test)

    # Save results
    results.append({
        'Model': model_name,
        'Best Scaler': best_scaler,
        'Best Parameters': best_params,
        'Train Accuracy': accuracy_score(y_train, y_train_pred),
        'Test Accuracy': accuracy_score(y_test, y_test_pred),
        'Train F1 Score': f1_score(y_train, y_train_pred, average='weighted'),
        'Test F1 Score': f1_score(y_test, y_test_pred, average='weighted')
    })

    # --- Step 4: Print important metrics ---
    print(f"\nüîç Model: {model_name}")
    print(f"Best Scaler: {best_scaler}")
    print(f"Best Parameters: {best_params}")
    print(f"Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}")
    print(f"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}")
    print(f"Train F1 Score: {f1_score(y_train, y_train_pred, average='weighted'):.4f}")
    print(f"Test F1 Score: {f1_score(y_test, y_test_pred, average='weighted'):.4f}")
    print("Classification Report (Test Set):")
    print(classification_report(y_test, y_test_pred))
    print("-" * 60)

# --- Step 5: Summary Table ---
print("\nüìã Final Summary Table:")
results_df = pd.DataFrame(results)
print(results_df)

end_time = time.time()
print(f"\n‚è±Ô∏è Total processing time: {end_time - start_time:.2f} seconds")